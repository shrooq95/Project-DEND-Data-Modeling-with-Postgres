# Data Modeling with Postgres & ETL Pipeline for Sparkify

## Introduction:
Sparkify is a music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, their data resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app. However, this cannot provid an easy way to query the data.

## Purpose: 
The purpose of this project is to create a Postgres database and ETL pipeline to optimize queries to help Sparkify's analytics team.

## File included: 
### data - Source of the JSON file, all these files have to be elaborated
      /log_data - A folder that contains files of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app                   activity logs from a music streaming app based on specified configurations.
      /song_data - Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of                     each song's track ID
    
### etl.ipynb - It is a notebook that helps to know step by step what etl.py does

### test.ipynb - It is a notebook that helps to know if tables are created and data are ingested correctly

### RunTables.ipynb - t is a notebook that helps to load and run the tables from create_tables.py file. 

### create_tables.py - This script will drop old tables (if exist) ad re-create new tables

### etl.py - This script will read JSON every file contained in /data folder, parse them, build relations though logical process and ingest data

### sql_queries.py - This file contains variables with SQL statement in String formats, partitioned by CREATE, DROP, INSERT statements plus a FIND query

